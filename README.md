# Machine Learning Model Comparison

## ğŸ“Œ Introduction
This project focuses on comparing different machine learning models, including Logistic Regression (plain, L1, L2) and ensemble methods (Bagging, Boosting, CatBoost).  
Both **from-scratch implementations** and **library-based implementations** were tested.  
The models were evaluated using metrics such as **Accuracy, F1 Score, and AUC** to identify the best-performing approaches.

---

## ğŸš€ Features
- Implemented Logistic Regression (without regularization, L1, L2)
- Implemented Bagging and Boosting from scratch
- Used library implementations for comparison (Scikit-learn, CatBoost)
- Preprocessing pipeline: handling missing values, one-hot encoding, scaling
- Model evaluation with multiple performance metrics
- Performance comparison table and analysis

---

## ğŸ› ï¸ Technologies Used
- Python 3.11.13  
- Pandas, NumPy  
- Scikit-learn   
- Matplotlib,seaborn (for visualization)

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ artifacts/ # Saved preprocessed numpy arrays
â”‚ â”œâ”€â”€ X_train.npz
â”‚ â”œâ”€â”€ X_test.npz
â”‚ â”œâ”€â”€ Y_train.npz
â”‚ â”œâ”€â”€ Y_test.npz
â”‚
â”œâ”€â”€ B_from_scratch_models/ # Custom from-scratch models
â”‚ â”œâ”€â”€ L1 Regularised Logistic Regression.ipynb
â”‚ â”œâ”€â”€ L2 Regularised Logistic Regression.ipynb
â”‚ â”œâ”€â”€ Simple Logistic Regression.ipynb
â”‚ â”œâ”€â”€ Simple Bagging decision tree.ipynb
â”‚ â”œâ”€â”€ Simple Boosting decision tree.ipynb
â”‚
â”œâ”€â”€ data/ # Dataset files
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â””â”€â”€ Titanic-Dataset.csv
â”‚ â”œâ”€â”€ processed/
â”‚ â””â”€â”€ titanicDataset_Final.csv
â”‚
â”œâ”€â”€ Section C - Library Implementations/ # Scikit-learn & CatBoost
â”‚ â”œâ”€â”€ A_minimal_prep_and_split.ipynb
â”‚ â”œâ”€â”€ C_library_models.ipynb
â”‚ â”œâ”€â”€ catboost_info/
â”œ
â”œâ”€â”€ requirments.txt

## Install Dependecies

pip install -r requirements.txt

